{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import requests\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from langchain_openai import ChatOpenAI\n",
    "import openai\n",
    "import re\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수에서 API 키 읽기\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# CLIP 모델과 프로세서 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# 실행 흐름\n",
    "input_file = \"output_data.jsonl\"\n",
    "keywords_file = \"keyword.json\"\n",
    "embedding_file = \"keyword_embeddings.json\"\n",
    "rag_dataset_file = \"rag_dataset.json\"\n",
    "username = \"KimEunOh\"\n",
    "repo = \"image\"\n",
    "branch = \"main\"\n",
    "folder_path = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. output_data로부터 assistant의 content값 추출\n",
    "def extract_assistant_content(input_file):\n",
    "    contents = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        for line in infile:\n",
    "            data = json.loads(line.strip())\n",
    "            for message in data.get(\"messages\", []):\n",
    "                if message.get(\"role\") == \"assistant\":\n",
    "                    content = message.get(\"content\", \"\").strip()\n",
    "                    if content:\n",
    "                        contents.append(content)\n",
    "    return contents\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 마크다운, 불필요한 기호, 공백을 제거\n",
    "    \"\"\"\n",
    "    # 1. 마크다운 포맷 제거\n",
    "    text = re.sub(r\"\\*\\*|\\*|__|_|`\", \"\", text)  # Bold, Italic, Underline, Inline Code\n",
    "    text = re.sub(r\"#+\\s?\", \"\", text)  # Header (#)\n",
    "\n",
    "    # 2. 특수 문자 제거\n",
    "    text = re.sub(r\"[()\\[\\]{}]\", \"\", text)  # 괄호 및 대괄호\n",
    "    text = re.sub(r\"[<>]\", \"\", text)  # 꺽쇠 괄호\n",
    "\n",
    "    # 3. 다중 공백과 줄바꿈 처리\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # 다중 공백 제거\n",
    "    text = text.strip()  # 양쪽 공백 제거\n",
    "\n",
    "    # 4. 빈 문자열 검사\n",
    "    if not text:\n",
    "        print(\"Warning: Text is empty after cleaning.\")\n",
    "    return text\n",
    "\n",
    "\n",
    "def truncate_text(text, max_length=1000):\n",
    "    \"\"\"\n",
    "    입력 텍스트 길이 조절.\n",
    "    \"\"\"\n",
    "    if len(text) > max_length:\n",
    "        return text[:max_length] + \"...\"\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_keywords_with_llm(contents, output_file):\n",
    "    \"\"\"\n",
    "    GPT를 사용하여 키워드를 추출하고 JSON 파일에 저장\n",
    "    \"\"\"\n",
    "    keywords_data = []\n",
    "    for idx, content in enumerate(contents):\n",
    "        cleaned_content = clean_text(content)\n",
    "        truncated_content = truncate_text(cleaned_content, max_length=1000)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI model trained to analyze text and extract deepfake indicators or clues. These clues can include unnatural features, inconsistencies, or anomalies commonly found in deepfake media.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "                Analyze the following text to extract key deepfake clues. Focus on identifying abnormalities or inconsistencies in facial features, textures, lighting, or expressions that might suggest digital manipulation.\n",
    "\n",
    "                Text:\n",
    "                {truncated_content}\n",
    "\n",
    "                Extracted Clues:\n",
    "                1. Identify any unnatural features, such as misaligned eyes, uneven lighting, or irregular skin texture.\n",
    "                2. Include specific details about abnormalities, such as asymmetrical facial features, inconsistent reflections, or poorly rendered details like teeth or hair.\n",
    "                3. Mention lighting inconsistencies, shadow mismatches, or environmental factors that suggest editing.\n",
    "                4. List these clues as specific and concise keywords separated by commas.\n",
    "\n",
    "                Keywords:\n",
    "                \"\"\",\n",
    "            },\n",
    "        ]\n",
    "        try:\n",
    "            gpt = ChatOpenAI(\n",
    "                temperature=0,\n",
    "                model_name=\"gpt-4o\",  # 모델명\n",
    "            )\n",
    "            response = gpt.invoke(messages)\n",
    "            keywords = response.content.strip()\n",
    "            print(f\"Keywords for item {idx}: {keywords}\")\n",
    "            keywords_data.append({\"content\": content, \"keywords\": keywords})\n",
    "        except KeyError as ke:\n",
    "            # 응답 형식이 예상과 다를 경우 오류 처리\n",
    "            print(f\"KeyError for item {idx}: {ke}\")\n",
    "            logging.error(f\"KeyError for item {idx}: {ke} - Response: {response}\")\n",
    "            keywords_data.append(\n",
    "                {\"content\": content, \"keywords\": \"default, keyword, placeholder\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # 기타 예외 처리\n",
    "            print(f\"Error extracting keywords for content at index {idx}: {e}\")\n",
    "            logging.error(f\"Error extracting keywords for content at index {idx}: {e}\")\n",
    "            keywords_data.append(\n",
    "                {\"content\": content, \"keywords\": \"default, keyword, placeholder\"}\n",
    "            )\n",
    "    # JSON 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(keywords_data, outfile, ensure_ascii=False, indent=4)\n",
    "    return keywords_data\n",
    "\n",
    "\n",
    "# 3. keyword.json을 CLIP를 통해 임베딩을 수행하고, DB에 저장\n",
    "def embed_keywords_with_clip(keyword_file, output_file):\n",
    "    with open(keyword_file, \"r\", encoding=\"utf-8\") as infile:\n",
    "        keywords_data = json.load(infile)\n",
    "\n",
    "    embeddings = []\n",
    "    for entry in keywords_data:\n",
    "        keywords = entry[\"keywords\"]\n",
    "        inputs = clip_processor(\n",
    "            text=[keywords], return_tensors=\"pt\", padding=True, truncation=True\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_embedding = clip_model.get_text_features(**inputs)\n",
    "        embeddings.append(\n",
    "            {\"keywords\": keywords, \"embedding\": text_embedding.cpu().tolist()}\n",
    "        )\n",
    "    # JSON 저장\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(embeddings, outfile, ensure_ascii=False, indent=4)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# 4. GitHub의 목록으로부터 이미지 URL 리스트 전달받음\n",
    "def get_github_image_urls(username, repo, branch, folder_path):\n",
    "    api_url = f\"https://api.github.com/repos/{username}/{repo}/contents/{folder_path}?ref={branch}\"\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        response.raise_for_status()\n",
    "        files = response.json()\n",
    "        image_urls = [\n",
    "            f\"https://raw.githubusercontent.com/{username}/{repo}/{branch}/{folder_path}/{file['name']}\"\n",
    "            for file in files\n",
    "            if file.get(\"name\", \"\").lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "        ]\n",
    "        return image_urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching image URLs from GitHub: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# 5. 이미지 임베딩을 수행하여, 기존에 저장된 텍스트 임베딩 값과 유사도 계산\n",
    "def embed_image_and_calculate_similarity(image_urls, keyword_embeddings):\n",
    "    results = []\n",
    "    for image_url in image_urls:\n",
    "        try:\n",
    "            response = requests.get(image_url)\n",
    "            response.raise_for_status()\n",
    "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "            inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                image_embedding = clip_model.get_image_features(**inputs).cpu()\n",
    "\n",
    "            # 유사도 계산\n",
    "            similarities = [\n",
    "                {\n",
    "                    \"keywords\": entry[\"keywords\"],\n",
    "                    \"similarity\": torch.nn.functional.cosine_similarity(\n",
    "                        torch.tensor(entry[\"embedding\"]), image_embedding\n",
    "                    ).item(),\n",
    "                }\n",
    "                for entry in keyword_embeddings\n",
    "            ]\n",
    "            most_relevant = sorted(\n",
    "                similarities, key=lambda x: x[\"similarity\"], reverse=True\n",
    "            )[:5]\n",
    "            results.append({\"image_url\": image_url, \"relevant_keywords\": most_relevant})\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_url}: {e}\")\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "\n",
    "# 6. 추출된 키워드들을 바탕으로 프롬프트 생성\n",
    "def generate_prompts(results):\n",
    "    prompts = []\n",
    "    for result in results:\n",
    "        image_url = result[\"image_url\"]\n",
    "        relevant_keywords = [kw[\"keywords\"] for kw in result[\"relevant_keywords\"]]\n",
    "        prompt = f\"\"\"\n",
    "            Analyze the image at the following URL: {image_url}.  \n",
    "            Using the following keywords as a checklist: {', '.join(relevant_keywords)}.  \n",
    "\n",
    "            Instructions:  \n",
    "            1. For each keyword, verify whether the described clue is present in the image. Clearly state which keywords (if any) correspond to observed anomalies.  \n",
    "            2. If no anomalies are found, conclude that the image does not show signs of deepfake manipulation.  \n",
    "            3. Provide a balanced judgment and explain your reasoning, ensuring not to assume manipulation unless supported by clear evidence.  \n",
    "\n",
    "            Response Format:  \n",
    "            1. Observations: List the observations for each keyword, explicitly noting whether it matches an observed anomaly or not.  \n",
    "            2. Conclusion: State whether the image is likely a deepfake (Yes/No).  \n",
    "            3. Explanation: Provide a brief explanation, citing specific observations or lack thereof.\n",
    "\n",
    "            Example Response:  \n",
    "            Observations:  \n",
    "            - \"Mismatched shadows\": Not observed.  \n",
    "            - \"Unnatural facial features\": Slight asymmetry in the eyes.  \n",
    "            - \"Smooth skin texture\": Observed on the cheeks.  \n",
    "\n",
    "            Conclusion: No.  \n",
    "\n",
    "            Explanation: While some minor anomalies were noted (e.g., asymmetry in the eyes), these are consistent with natural variations in human features and do not strongly indicate manipulation.\n",
    "            \"\"\"\n",
    "        prompts.append({\"image_url\": image_url, \"prompt\": prompt.strip()})\n",
    "    return prompts\n",
    "\n",
    "\n",
    "# 7. 이미지 URL과 프롬프트를 쌍으로 한 rag_dataset.json 파일 생성\n",
    "def save_rag_dataset(prompts, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        json.dump(prompts, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "새로운 output_data가 추가되었을 경우에만 실행 cntl + /\n",
    "\"\"\"\n",
    "\n",
    "# # 단계별 실행\n",
    "\n",
    "# # 컨텐츠 추출\n",
    "\n",
    "# assistant_contents = extract_assistant_content(input_file)\n",
    "\n",
    "# # 키워드 생성\n",
    "\n",
    "# keywords_data = extract_keywords_with_llm(assistant_contents, keywords_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키워드 임베딩\n",
    "keyword_embeddings = embed_keywords_with_clip(keywords_file, embedding_file)\n",
    "# 이미지 리스트 생성\n",
    "\n",
    "image_urls = get_github_image_urls(username, repo, branch, folder_path)\n",
    "\n",
    "# 이미지 임베딩 및 유사도 계산\n",
    "\n",
    "results = embed_image_and_calculate_similarity(image_urls, keyword_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG dataset saved to rag_dataset.json\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 생성\n",
    "prompts = generate_prompts(results)\n",
    "save_rag_dataset(prompts, rag_dataset_file)\n",
    "\n",
    "print(f\"RAG dataset saved to {rag_dataset_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biztour-2MXTw1DY-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
